{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clusterai2021_clase09_Redes_Neuronales_Clasifacion_PyTorch","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BHzfMHbFqIKi"},"source":["# ClusterAI 2021\n","\n","# Ciencia de Datos - Ingeniería Industrial - UTN BA\n","\n","# clase_09: Practica Redes Neuronales\n","\n","### Elaborado por: Aguirre Nicolas"]},{"cell_type":"markdown","metadata":{"id":"S8Y4IUeNrTH4"},"source":["# IMPORTS"]},{"cell_type":"code","metadata":{"id":"zj2VEgN6qjDJ"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","pd.set_option('display.float_format', lambda x: '%.1d' % x) # Para acotar los decimales en pandas"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shZj2OGKs0Hl"},"source":["# IRIS DATASET\n","\n","En esta primera ejercitacion vamos a retomar el dataset Iris (visto en la Clase 4)\n","\n","\n","El conjunto de datos contiene 50 muestras de cada una de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midió cuatro rasgos de cada muestra: lo largo y lo ancho del sépalos y pétalos, en centímetros. Basado en la combinación de estos cuatro rasgos.\n","\n","https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos\n","\n","Los datos son:\n","\n","| Columna | Descripcion |\n","| --- | --- |\n","| ID | Unique ID |\n","| SepalLengthCm | Length of the sepal (cm) |\n","| SepalWidthCm | Width of the sepal (cm) |\n","| PetalLengthCm | Length of the petal (cm) |\n","| PetalWidthCm | Width of the petal (cm) |\n","| Species | name |\n"]},{"cell_type":"code","metadata":{"id":"GxUnNijVsmww"},"source":["# Primero cargamos los datos que ya vienen incluidos en la libreria sk-learn.\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","X = iris.data\n","Y = iris.target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIbBihoeU95Q"},"source":["n_features = np.shape(X)[0]\n","n_samples = np.shape(X)[1]\n","n_classes = np.unique(Y)\n","print(f'Features: ',n_samples)\n","print(f'Samples: ',n_features)\n","print(f'Classes: ',n_classes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hhudQ_Xs1yL"},"source":["# Veamos la primera sample\n","print(f'X: {X[0]}')\n","print(f'Y: {Y[0]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"--P7frv5qX51"},"source":["## SPLIT"]},{"cell_type":"code","metadata":{"id":"uSP2IQWiq91_"},"source":["# Separamos train y test set\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n","# Separamos train y validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFMeW20jqa52"},"source":["## SCALING"]},{"cell_type":"code","metadata":{"id":"UReu6invWAGg"},"source":["# Noralizamos\n","scaler = preprocessing.StandardScaler()\n","scaler.fit(x_train)\n","x_train_norm = scaler.transform(x_train)\n","x_val_norm = scaler.transform(x_val)\n","x_test_norm = scaler.transform(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPCxvrLMtFYc"},"source":["# Modelo"]},{"cell_type":"code","metadata":{"id":"Dv5XmStNuTP0"},"source":["# Pytorch\n","import torch\n","print('Version de Pytorch: ',torch.__version__)\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset,Dataset, DataLoader\n","import torch.nn.functional as F\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x4wHwXuVtPg9"},"source":["## Modelo"]},{"cell_type":"markdown","metadata":{"id":"LJfJ3XbtEfoa"},"source":["Al igual que TensorFlow-Keras, PyTorch es una libreria para codear modelos de NN y tambien tiene su modelo Sequential.\n","\n","Pero a diferencia de TF-Keras, PyTorch no tiene nativamente las funciones **fit**, **evaluate** y **predict**.\n","\n","Por lo cual somos nosotros quienes vamos a tener codear el entrenamiento ..."]},{"cell_type":"markdown","metadata":{"id":"3_L2XfJcEF9Q"},"source":["El tipo de arquitectura que vamos a utilizar es una NN *fully-conected*. Para esto utilizaremos la funcion *nn.Sequential()* a la cual le pasaremos los distintos componentes de la red.\n","\n","Cuando usamos una arquitectura secuencial, vamos construyendo el **foreward pass** de la red de manera tal que la red utiliza la salida de la capa inmediatamente anterior en la capa siguiente, y asi sucecivamente hasta llegar al ultimo elemento de la red con el cual generamos el output.\n","\n","Puntualmente, en este ejemplo utilizaremos unicamente dos componentes.\n","\n","* [**nn.Linear**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) : Para aplicar la transformacion lineal $z = xW^T + b$\n","\n","* [**nn.Sigmoid**](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) : Para aplicar la funcion $\\sigma(x) = \\frac{1}{1 + \\exp(-z)}$\n","\n","En donde $W^T$ son los pesos (weights) que aprendera la red.\n","\n","Cada una de las capas de nn.Linear se construye dandole la informacion de la cantidad de features de entrada y de salida. Como en nuestro ejemplo tenemos 3 clases de flores, la ultima capa debera tener una salida de dimension = 3.\n"]},{"cell_type":"code","metadata":{"id":"sdKlnGr2tFEd"},"source":["input_features = 4\n","layers_features = 4\n","output_dim = 3\n","model = nn.Sequential(\n","          nn.Linear(input_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,output_dim)\n","          )         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90n1GM9SvPUK"},"source":["model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RlSnpr9ICHOA"},"source":["## Optimizador y Loss function"]},{"cell_type":"code","metadata":{"id":"0QH9v8SqvYuX"},"source":["# Learning rate y Optimizador\n","lr = 0.05\n","optimizador = torch.optim.SGD(model.parameters(),lr=lr)\n","# Funcion de penalizacion\n","loss_func = nn.CrossEntropyLoss() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQqCg3n74MbG"},"source":["# Tensor Dataset"]},{"cell_type":"markdown","metadata":{"id":"xse7iLVFH84T"},"source":["En general, cuando entrenamos NN los datos deben estar contenidos en tensores. \n","Un tensor es una generalización de los vectores y las matrices y se entiende fácilmente como una matriz multidimensional. \n","\n","Ejemplo:\n","\n","1. Un vector es un 1D tensor de dimension $1x5$\n","\n","2. Una matriz es un 2D tensor de dimension $2x5$\n","\n","En particular, dentro del área de Deep Learning se llama Tensor a aquellas estructura de datos que son capaces de realizar operaciones en **paralelo** dentro de las GPUs, aumentando significativamente nuestra capacidad de computo."]},{"cell_type":"markdown","metadata":{"id":"4YcFpOAjK99Y"},"source":["Por ultimo deberemos definir el generador de datos para entrenar en batches. Estos objetos se llaman Dataloaders. \n","\n","Para crearlo le pasamos nuestro dataset y el tamaño de nuestro batch."]},{"cell_type":"code","metadata":{"id":"zrWi1A7GwDFB"},"source":["# Batch size\n","bs = 8\n","\n","# Pasamos nuestro numpy arrays, a Tensor\n","x_train_norm_t, y_train_t = torch.Tensor(x_train_norm), torch.LongTensor(y_train) # Los Long Tensor son int\n","x_val_norm_t, y_val_t = torch.Tensor(x_val_norm), torch.LongTensor(y_val)\n","x_test_norm_t, y_test_t = torch.Tensor(x_test_norm), torch.LongTensor(y_test)\n","\n","# Creamos los Dataset\n","train_ds = TensorDataset(x_train_norm_t,y_train_t)\n","val_ds = TensorDataset(x_val_norm_t,y_val_t) \n","test_ds = TensorDataset(x_test_norm_t,y_test_t) \n","\n","# Creamos los Dataloader\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","val_dl = DataLoader(val_ds, batch_size=bs, shuffle=True) \n","test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCFJnJ86Bpmt"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"v65OgSpRGM0q"},"source":["En este punto ya contamos con todas las \"herramientras\" que necesitamos para entrenar un modelo de NN.\n","\n","* Datos\n","* Optimizador\n","* Funcion de Penalizacion\n","* Red Neuronal\n","\n","Vamos a ver como es que esto elementos \"interactuan\" para actualizar los parametros/*weights* de la Red Neuronal ... es decir ... para entrenarla."]},{"cell_type":"markdown","metadata":{"id":"pAwFcarcVMsH"},"source":["![](https://drive.google.com/uc?id=1d7Rq5FsmQNWcP8T1KGi_TodL49jCEKnY)"]},{"cell_type":"code","metadata":{"id":"NapSjQIkwDD4"},"source":["# Cantidad de Epochs\n","n_epochs = 900\n","\n","# Generamos un diccionario donde vamos a guaradr historial de entrenamiento\n","training = {'train':{'loss':[],                             \n","                    'acc':[]},\n","           'val':{'loss':[],\n","                  'acc':[]}\n","           }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJx1qPJ3wDAu"},"source":["# For loop epochs\n","for epoch in range(n_epochs):\n","  # Ahora nosotros vamos a tener que definir estas variables ...\n","  train_loss = 0\n","  cls_correctas_train = 0\n","  train_acc = 0\n","  ################################################\n","  # TRAINING\n","  ################################################\n","  for i , (x_batch, y_batch) in enumerate(train_dl):\n","    # Limpiamos todos los gradientes cargados en el optimizador\n","    optimizador.zero_grad()\n","    # Con un x_batch generamos una prediccion\n","    y_pred = model(x_batch)\n","    # La clase predicha será el índice de máximo valor luego del \n","    # softmax (integrado en CrossEntropyLoss)\n","    _, predicted = torch.max(y_pred.data, 1)\n","    # Calculamos la loss\n","    batch_loss = loss_func(y_pred,y_batch)\n","    # Calculamos el gradiente de la loss\n","    # Aca es donde sucede el back-propagation a.k.a. \"la magia\"\n","    batch_loss.backward() \n","    # Ajustamos los parametros del modelo con el optimizador\n","    optimizador.step()\n","    # Acumulamos la loss\n","    train_loss += batch_loss.item()\n","    # Sumamos la cantidad de clases correctas en el batch\n","    correct_i = (predicted == y_batch).sum().item()\n","    # Acumulamos corrects\n","    cls_correctas_train += correct_i\n","\n","  ################################################\n","  # VALIDATION\n","  ################################################\n","  val_loss = 0\n","  cls_correctas_val = 0\n","  val_acc = 0\n","  with torch.torch.inference_mode():\n","    for x_batch, y_batch in val_dl:\n","      y_pred = model(x_batch)\n","      _, predicted = torch.max(y_pred.data, 1)\n","      batch_loss = loss_func(y_pred,y_batch)\n","      val_loss += batch_loss.item()\n","      correct_i = (predicted == y_batch).sum().item()\n","      cls_correctas_val += correct_i\n","\n","  # Calculamos el accuracy\n","  train_acc = (cls_correctas_train / len(train_ds))*100    # Calculamos el accuracy\n","  val_acc = (cls_correctas_val / len(val_ds))*100  \n","\n","  # Imprimimos en pantalla\n","  print('Epoch: {} T_Loss: {:.4f} T_Acc: {:.2f}%  V_Loss: {:.4f} V_Acc: {:.2f} %'.format(\n","      epoch,train_loss,train_acc,val_loss,val_acc))\n","\n","  # Guardamos en el historial de entrenamiento ...\n","  training['train']['loss'].append(train_loss)\n","  training['train']['acc'].append(train_acc)\n","  training['val']['loss'].append(val_loss)\n","  training['val']['acc'].append(val_acc)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcTsBG9UAYJe"},"source":["#Loss\n","train_loss_h = training['train']['loss']\n","val_loss_h = training['val']['loss']\n","# Acc\n","train_acc_h = training['train']['acc']\n","val_acc_h = training['val']['acc']\n","lepochs = range(1, len(train_loss_h) + 1)\n","\n","fig, axs = plt.subplots(1,2,figsize=(16,6))\n","axs[0].plot(lepochs, train_loss_h, 'b', label='Train loss')\n","axs[0].plot(lepochs, val_loss_h, 'r', label='Val loss')\n","axs[0].set_title('Training and validation Loss',fontsize=20)\n","axs[0].legend(fontsize=16)\n","axs[1].plot(lepochs, train_acc_h, 'b', label='Train Accuracy')\n","axs[1].plot(lepochs, val_acc_h, 'r', label='Validation Accuracy')\n","axs[1].set_title('Training and validation Loss',fontsize=20)\n","axs[1].legend(fontsize=16)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79yqU-Uunu5O"},"source":["# **PREGUNTAS**:\n","```\n","1) Es correcto que nuestra loss de train y de validacion disminuyan, y sin embargo el accuracy se mantenga igual/baje ? Por que ? \n","\n","2) Que cambios podriamos hacer para intentar solucionar el entrenamiento con la sigmoid function?\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"US03jef4exdE"},"source":["# DESAFIO"]},{"cell_type":"markdown","metadata":{"id":"PvxWJk1ve6VV"},"source":["Generar una funcion llamada **func_train()** y **func_val()** para reemplezar en el for loop de entrenamiento.\n","\n","Luego, generar una fucion llamada **func_fit()** la cual reemplace todo el entrenamiento y validacion (debe contener dentro de si **func_train()** y **func_val()** ).\n","\n","Ayuda: \n","\n","func_fit() debe recibir como argumentos (inputs)\n","\n","* el modelo, optimizador, loss function, dataloaders, datasets y la cantidad de epochs a entrenar\n","\n","* el return de la funcion es el modelo ya entrenado y el historial de entrenamiento.\n","\n"]},{"cell_type":"code","metadata":{"id":"oF8i9BUie-kD"},"source":["def func_train(model, train_dl,train_ds, loss_func, optimizador):\n","  \n","  ############################\n","  # CODIGO AQUI\n","  ############################\n","\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNJUtIYyhkzO"},"source":["def func_validation(model, val_dl,val_ds,loss_func):\n","  \n","  ############################\n","  # CODIGO AQUI\n","  ############################\n","  \n","  return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Ufv-Lyqhms-"},"source":["def func_fit(n_epochs,model,\n","             train_dl,train_ds,val_dl,val_ds,\n","             loss_func,optimizador):  \n","  \n","  ############################\n","  # CODIGO AQUI\n","  ############################\n","  \n","  #Ayuda: en algun momento debemos llamar a func_train() y luego llamar a func_val()\n","  \n","  return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77gbWM6CiwAs"},"source":["# Cantidad de Epochs a entrenar\n","n_epochs = 800 \n","\n","func_fit( n_epochs,model,train_dl,train_ds,val_dl,val_ds,loss_func,optimizador)"],"execution_count":null,"outputs":[]}]}